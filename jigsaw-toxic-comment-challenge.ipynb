{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport re\nimport torch\nimport spacy\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import Counter\nfrom sklearn.metrics import roc_auc_score,accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T03:14:13.270720Z","iopub.execute_input":"2022-06-30T03:14:13.271269Z","iopub.status.idle":"2022-06-30T03:14:17.063869Z","shell.execute_reply.started":"2022-06-30T03:14:13.271171Z","shell.execute_reply":"2022-06-30T03:14:17.062827Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:17.069215Z","iopub.execute_input":"2022-06-30T03:14:17.072371Z","iopub.status.idle":"2022-06-30T03:14:18.718826Z","shell.execute_reply.started":"2022-06-30T03:14:17.072329Z","shell.execute_reply":"2022-06-30T03:14:18.717943Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Label values with -1 mean that this row was not used for scoring in the competition","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:20.876423Z","iopub.execute_input":"2022-06-30T03:14:20.877055Z","iopub.status.idle":"2022-06-30T03:14:20.882080Z","shell.execute_reply.started":"2022-06-30T03:14:20.877014Z","shell.execute_reply":"2022-06-30T03:14:20.880463Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ntest_labels = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\ntest_df = pd.merge(test, test_labels, left_on='id', right_on='id', how='left')\ndrop_idxs = test_df[\n    (test_df.toxic == -1) | (test_df.severe_toxic == -1) | (test_df.obscene == -1) | \n    (test_df.threat == -1) | (test_df.insult == -1) | (test_df.identity_hate == -1)\n].index\ntest_df = test_df.drop(drop_idxs, axis=\"rows\")\n\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:21.183385Z","iopub.execute_input":"2022-06-30T03:14:21.183729Z","iopub.status.idle":"2022-06-30T03:14:22.926604Z","shell.execute_reply.started":"2022-06-30T03:14:21.183698Z","shell.execute_reply":"2022-06-30T03:14:22.925663Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test_df[test_df.toxic == 1]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:23.024456Z","iopub.execute_input":"2022-06-30T03:14:23.025134Z","iopub.status.idle":"2022-06-30T03:14:23.043731Z","shell.execute_reply.started":"2022-06-30T03:14:23.025097Z","shell.execute_reply":"2022-06-30T03:14:23.042809Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning Pipeline","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text, lower_case=True, clean_text=True):\n    \n    if lower_case:\n        text = text.lower()\n    \n    # Remove website links\n    template = re.compile(r'https?://\\S+|www\\.\\S+') \n    text = template.sub(r'', text)\n    \n    # Remove HTML tags\n    template = re.compile(r'<[^>]*>') \n    text = template.sub(r'', text)\n    \n    # Remove none ascii characters\n    template = re.compile(r'[^\\x00-\\x7E]+') \n    text = template.sub(r'', text)\n    \n    # Replace none printable characters\n    template = re.compile(r'[\\x00-\\x0F]+') \n    text = template.sub(r' ', text)\n    \n    if clean_text:\n        # Remove special characters\n        text = re.sub(\"'s\", '', text)\n        template = re.compile('[\"#$%&\\'()\\*\\+-/:;<=>@\\[\\]\\\\\\\\^_`{|}~]') \n        text = template.sub(r' ', text)\n        # Replace multiple punctuation \n        text = re.sub('[.!?]{2,}', '.', text)\n        text = re.sub(',+', ',', text) \n        # Remove numbers\n        text = re.sub('\\d+', ' ', text) \n        \n    # Remove extra spaces\n    text = re.sub('\\s+', ' ', text)\n    \n    # Remove spaces at the beginning and at the end of string\n    text = text.strip() \n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:31.719515Z","iopub.execute_input":"2022-06-30T03:14:31.720174Z","iopub.status.idle":"2022-06-30T03:14:31.730233Z","shell.execute_reply.started":"2022-06-30T03:14:31.720138Z","shell.execute_reply":"2022-06-30T03:14:31.727957Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Cleaning the comments","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:05:55.855377Z","iopub.execute_input":"2022-06-26T04:05:55.855791Z","iopub.status.idle":"2022-06-26T04:05:55.859924Z","shell.execute_reply.started":"2022-06-26T04:05:55.855742Z","shell.execute_reply":"2022-06-26T04:05:55.859116Z"}}},{"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].map(lambda com : preprocess_text(com))\ntest_df[\"comment_text\"] = test_df[\"comment_text\"].map(lambda com : preprocess_text(com))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:33.124933Z","iopub.execute_input":"2022-06-30T03:14:33.125602Z","iopub.status.idle":"2022-06-30T03:14:57.078930Z","shell.execute_reply.started":"2022-06-30T03:14:33.125568Z","shell.execute_reply":"2022-06-30T03:14:57.077944Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Distribution of Labels**","metadata":{}},{"cell_type":"code","source":"train_df[['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']].sum(axis =0 )","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:57.080887Z","iopub.execute_input":"2022-06-30T03:14:57.081526Z","iopub.status.idle":"2022-06-30T03:14:57.096357Z","shell.execute_reply.started":"2022-06-30T03:14:57.081488Z","shell.execute_reply":"2022-06-30T03:14:57.095137Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:57.097985Z","iopub.execute_input":"2022-06-30T03:14:57.098815Z","iopub.status.idle":"2022-06-30T03:14:57.105405Z","shell.execute_reply.started":"2022-06-30T03:14:57.098777Z","shell.execute_reply":"2022-06-30T03:14:57.104247Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Getting word dictionary/Max Sentence Length","metadata":{}},{"cell_type":"code","source":"def get_dict(df_clean):\n    \n    reviews = [str(review).split(' ') for review in list(df_clean['comment_text'] )]\n    word_freq = Counter([token for review in reviews for token in review]).most_common()\n\n    word_freq = dict(word_freq)\n    min_freq = 5\n    word_dict = {}\n\n\n    # sending all the unknowns to 0\n    i = 1\n    for word in word_freq:\n        if word_freq[word] > min_freq:\n            word_dict[word] = i\n            i += 1\n        else:\n            word_dict[word] = 0\n\n\n    max_length = 0\n    for idx in tqdm(range(len(df_clean))):\n        row = df_clean.iloc[idx]\n        length = len(str(row['comment_text']).split(' '))\n        if length > max_length:\n            max_length = length\n            \n    return max_length+1, word_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:57.108567Z","iopub.execute_input":"2022-06-30T03:14:57.109175Z","iopub.status.idle":"2022-06-30T03:14:57.118429Z","shell.execute_reply.started":"2022-06-30T03:14:57.109132Z","shell.execute_reply":"2022-06-30T03:14:57.117339Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"max_length, word_dict = get_dict(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:14:57.120183Z","iopub.execute_input":"2022-06-30T03:14:57.121059Z","iopub.status.idle":"2022-06-30T03:15:21.218532Z","shell.execute_reply.started":"2022-06-30T03:14:57.120995Z","shell.execute_reply":"2022-06-30T03:15:21.217452Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"LABEL_COLUMNS = train_df.columns.tolist()[2:]\nLABEL_COLUMNS","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:15:21.220064Z","iopub.execute_input":"2022-06-30T03:15:21.221075Z","iopub.status.idle":"2022-06-30T03:15:21.228394Z","shell.execute_reply.started":"2022-06-30T03:15:21.221034Z","shell.execute_reply":"2022-06-30T03:15:21.227231Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Creating Dataset","metadata":{}},{"cell_type":"code","source":"class ToxicComment_Dataset(Dataset):\n    def __init__(self, df,word_dict, max_length):\n        self.df = df\n        self.word_dict = word_dict\n        self.max_len = max_length\n        self.UNK = max(word_dict.values())\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        labels = row[LABEL_COLUMNS]\n        review = str(row['comment_text']).split(' ')\n        x = torch.zeros(self.max_len)\n        for idx in range(len(review)):\n            x[self.max_len - len(review) + idx] = self.word_dict.get(review[idx], self.UNK)\n        \n        label = labels=torch.FloatTensor(labels)\n        #torch.tensor(row_label['is_sarcastic']).float()\n        return x.long(), label\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:15:21.229790Z","iopub.execute_input":"2022-06-30T03:15:21.231014Z","iopub.status.idle":"2022-06-30T03:15:21.241643Z","shell.execute_reply.started":"2022-06-30T03:15:21.230975Z","shell.execute_reply":"2022-06-30T03:15:21.240587Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Loading from Gensim Word Embedding","metadata":{}},{"cell_type":"code","source":"import gensim.downloader\nglove_emb = gensim.downloader.load('glove-wiki-gigaword-100')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:15:21.243807Z","iopub.execute_input":"2022-06-30T03:15:21.244923Z","iopub.status.idle":"2022-06-30T03:16:15.179660Z","shell.execute_reply.started":"2022-06-30T03:15:21.244863Z","shell.execute_reply":"2022-06-30T03:16:15.178673Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"weights = glove_emb.get_normed_vectors()\nunk = weights.mean(axis = 0)\nall_weights = np.vstack((weights, unk))\n\ndict_length, word_dict = all_weights.shape[0], glove_emb.key_to_index","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:16:15.181259Z","iopub.execute_input":"2022-06-30T03:16:15.181628Z","iopub.status.idle":"2022-06-30T03:16:15.462693Z","shell.execute_reply.started":"2022-06-30T03:16:15.181588Z","shell.execute_reply":"2022-06-30T03:16:15.461700Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Creating Dataloader","metadata":{}},{"cell_type":"code","source":"def create_dataloader(X_train,X_test,word_dict, batch_size, max_length):\n    ds_train = ToxicComment_Dataset(X_train,word_dict, max_length)\n    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n\n\n    ds_test = ToxicComment_Dataset(X_test,word_dict, max_length)\n    dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n    return dl_train, dl_test, ds_train, ds_test\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:17:21.157013Z","iopub.execute_input":"2022-06-30T03:17:21.157689Z","iopub.status.idle":"2022-06-30T03:17:21.163738Z","shell.execute_reply.started":"2022-06-30T03:17:21.157652Z","shell.execute_reply":"2022-06-30T03:17:21.162731Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"batch_size = 1000\ndl_train, dl_test, ds_train, ds_test = create_dataloader(train_df,test_df,word_dict, batch_size, max_length)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:17:21.523361Z","iopub.execute_input":"2022-06-30T03:17:21.524339Z","iopub.status.idle":"2022-06-30T03:17:21.544714Z","shell.execute_reply.started":"2022-06-30T03:17:21.524289Z","shell.execute_reply":"2022-06-30T03:17:21.543591Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Creating Model - GRU","metadata":{}},{"cell_type":"code","source":"class GRU(nn.Module):\n    def __init__(self, weights, embedding_size,hidden_size):\n        super(GRU, self).__init__()\n        # padding index turns off gradient for unknown tokens\n        #self.word_emb = nn.Embedding(dict_length, embedding_size, padding_idx=0)\n        self.word_emb = nn.Embedding.from_pretrained(torch.tensor(weights), freeze=True)\n        self.gru = nn.GRU(input_size= embedding_size, hidden_size=100, num_layers=1, batch_first=True)\n        self.g = nn.Linear(hidden_size, 6)\n        \n        # PyTorch RNN outputs a sequence of same length as input\n        # For many to one, we can either use the final hidden state OR\n        # slap a linear layer on the output, taking in all the hidden states\n        \n    def forward(self, x):\n        #print(x.shape)\n        x = self.word_emb(x)\n        #print(x.shape)\n        # RNN layer outputs a tuple, the output and the final hidden state\n        # taking the final hidden state as output\n        x = self.gru(x)[1]\n        #print(x.shape)\n        output = self.g(x)\n        #output = torch.sigmoid(output)\n        #print(x.shape)\n        \n        return torch.squeeze(output)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:40:48.921545Z","iopub.execute_input":"2022-06-30T03:40:48.921985Z","iopub.status.idle":"2022-06-30T03:40:48.932252Z","shell.execute_reply.started":"2022-06-30T03:40:48.921951Z","shell.execute_reply":"2022-06-30T03:40:48.931157Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"x,y = next(iter(dl_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:40:49.290697Z","iopub.execute_input":"2022-06-30T03:40:49.291789Z","iopub.status.idle":"2022-06-30T03:40:50.233591Z","shell.execute_reply.started":"2022-06-30T03:40:49.291741Z","shell.execute_reply":"2022-06-30T03:40:50.232608Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Open pass and ROC","metadata":{}},{"cell_type":"code","source":"def one_pass(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False):\n    \n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    \n    if backwards == True:\n        model.train()\n    else:\n        model.eval()\n    \n    total_loss = 0.0    \n\n    all_y = []\n    all_y_hat = []\n    for x, y in tqdm(dataloader):\n        x = x.to(device)\n        y = y.to(device)\n        y_pred = model(x)\n        all_y.append(y)\n        all_y_hat.append(y_pred)\n        \n        loss = lossFun(y_pred, y)\n        total_loss += loss.item()\n        \n        if backwards == True:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    avg_loss = total_loss / len(dataloader)\n    \n    y = torch.cat(all_y,dim=0)\n    y_hat = torch.cat(all_y_hat,dim=0)  \n    roc = roc_auc_score(y.cpu(),y_hat.sigmoid().detach().cpu(), average = 'micro')\n    if print_loss == True:\n        print(avg_loss)\n    \n    return avg_loss, roc\n\ndef one_pass_roc(model, dataloader, num_points):\n    model.eval()\n    total_incorrect = 0\n    all_y = []\n    all_y_hat = []\n    \n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    \n    for x, y in dataloader:\n        x = x.to(device)\n        y = y.to(device)\n        y_pred = model(x)\n        \n        all_y.append(y)\n        all_y_hat.append(y_pred)\n        \n        #y_pred = (torch.sigmoid(model(x)) > 0.5).float()\n        \n        #total_incorrect += torch.count_nonzero(y - y_pred).item()\n    y = torch.cat(all_y,dim=0)\n    y_hat = torch.cat(all_y_hat,dim=0)  \n    roc = roc_auc_score(y.cpu(),y_hat.sigmoid().detach().cpu(), average = 'micro')\n    \n    return roc","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:40:51.779628Z","iopub.execute_input":"2022-06-30T03:40:51.780009Z","iopub.status.idle":"2022-06-30T03:40:51.794145Z","shell.execute_reply.started":"2022-06-30T03:40:51.779977Z","shell.execute_reply":"2022-06-30T03:40:51.792957Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Train the Modeel","metadata":{}},{"cell_type":"code","source":"dict_length = max(word_dict.values()) + 1\n#model = GRU(dict_length, 100, 100)\nmodel = GRU(weights, 100, 100)\nlossFun = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr = 0.01)\nmodel = model.cuda()\ncriterion = lossFun.cuda()\n\nnum_epochs = 5\n\nfor epoch in tqdm(range(num_epochs)):\n    print('Epoch: ', epoch)\n    \n    loss_train,roc_train = one_pass(model, dl_train, optimizer, lossFun)\n    loss_test, roc_test = one_pass(model, dl_test, optimizer, lossFun)\n    \n    #roc_train = one_pass_roc(model, dl_train, len(ds_train))\n    #roc_test = one_pass_roc(model, dl_test, len(ds_test))\n    print(f'Train loss: {loss_train} and Test loss: {loss_test}', )\n    print(f'Train ROC: {roc_train} and Test ROC: {roc_test}', )\n    #print(f'Train accuracy: {acc_train} and Test accuracy: {acc_test}', )","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:40:53.087338Z","iopub.execute_input":"2022-06-30T03:40:53.089217Z","iopub.status.idle":"2022-06-30T04:01:26.958087Z","shell.execute_reply.started":"2022-06-30T03:40:53.089168Z","shell.execute_reply":"2022-06-30T04:01:26.956640Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Creating Model - LSTM","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, weights, embedding_size,hidden_size):\n        super(LSTM, self).__init__()\n        # padding index turns off gradient for unknown tokens\n        #self.word_emb = nn.Embedding(dict_length, embedding_size, padding_idx=0)\n        self.word_emb = nn.Embedding.from_pretrained(torch.tensor(weights), freeze=True)\n        self.lstm = nn.LSTM(input_size= embedding_size, hidden_size=100, num_layers=1, batch_first=True)\n        self.g = nn.Linear(hidden_size, 6)\n        \n        # PyTorch RNN outputs a sequence of same length as input\n        # For many to one, we can either use the final hidden state OR\n        # slap a linear layer on the output, taking in all the hidden states\n        \n    def forward(self, x):\n        #print(x.shape)\n        x = self.word_emb(x)\n        out, hidden = self.lstm(x)\n        x = hidden[0]\n        #print(x.shape)\n        # RNN layer outputs a tuple, the output and the final hidden state\n        # taking the final hidden state as output\n        #x = self.gru(x)[1]\n        #print(x.shape)\n        output = self.g(x)\n        #output = torch.sigmoid(output)\n        #print(x.shape)\n        \n        return torch.squeeze(output)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:02:51.125630Z","iopub.execute_input":"2022-06-30T04:02:51.126008Z","iopub.status.idle":"2022-06-30T04:02:51.134548Z","shell.execute_reply.started":"2022-06-30T04:02:51.125977Z","shell.execute_reply":"2022-06-30T04:02:51.133351Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Train LSTM","metadata":{}},{"cell_type":"code","source":"dict_length = max(word_dict.values()) + 1\n#model = GRU(dict_length, 100, 100)\nmodel = LSTM(weights, 100, 100)\nlossFun = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr = 0.01)\nmodel = model.cuda()\ncriterion = lossFun.cuda()\n\nnum_epochs = 5\n\nfor epoch in tqdm(range(num_epochs)):\n    print('Epoch: ', epoch)\n    \n    loss_train,roc_train = one_pass(model, dl_train, optimizer, lossFun)\n    loss_test, roc_test = one_pass(model, dl_test, optimizer, lossFun)\n    \n    #roc_train = one_pass_roc(model, dl_train, len(ds_train))\n    #roc_test = one_pass_roc(model, dl_test, len(ds_test))\n    print(f'Train loss: {loss_train} and Test loss: {loss_test}', )\n    print(f'Train ROC: {roc_train} and Test ROC: {roc_test}', )\n    #print(f'Train accuracy: {acc_train} and Test accuracy: {acc_test}', )","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:02:52.564405Z","iopub.execute_input":"2022-06-30T04:02:52.564757Z","iopub.status.idle":"2022-06-30T04:23:59.446937Z","shell.execute_reply.started":"2022-06-30T04:02:52.564728Z","shell.execute_reply":"2022-06-30T04:23:59.445947Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}